{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a2b184",
   "metadata": {},
   "source": [
    "### Task: Neural Style Transfer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91fffb",
   "metadata": {},
   "source": [
    "#### For this task, I have selected two datasets. First, the dataset of best artworks of all time, and other is the dataset of Images of Dragon Ball Z characters. I will choose a style image from the art dataset and train the CNN network to transfer its style to the anime characters' images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babb7d50",
   "metadata": {},
   "source": [
    "I have used the autoencoder architecture in which the original image will be taken as an input, and the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695ad572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awast\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc18b4",
   "metadata": {},
   "source": [
    "### Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23bcb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "\n",
    "        files = os.listdir(folder_path)\n",
    "        image_files = [file for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        self.images = [Image.open(os.path.join(folder_path, image_file)) for image_file in image_files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "# Define a transformation to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "]) \n",
    "\n",
    "# Create a custom dataset\n",
    "dataset = ContentDataset(folder_path=\"C:\\\\Users\\\\awast\\\\Downloads\\\\archive_3\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404d5689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3145"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size=len(dataset)\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf905027",
   "metadata": {},
   "source": [
    "#### Importing Pretrained VGG network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855c7a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061da61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_image(tensor):\n",
    "#     if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "#         _log_api_usage_once(save_image)\n",
    "#     grid = make_grid(tensor, **kwargs)\n",
    "    # Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\n",
    "    ndarr = tensor.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    plt.imshow(im)\n",
    "\n",
    "# Load image file\n",
    "def load_image(path):\n",
    "    # Images loaded as BGR\n",
    "    img = cv2.imread(path)\n",
    "    return img\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# image_size = 224\n",
    "\n",
    "\n",
    "\n",
    "# original_img = dataset[0]\n",
    "# style_img = load_image(styles[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8eaad",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353339c",
   "metadata": {},
   "source": [
    "#### I have used an encoder decoder architecture, which will take the original image as the input and output the stylized image as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm=\"instance\"):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        # Padding Layers\n",
    "        padding_size = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n",
    "\n",
    "        # Convolution Layer\n",
    "        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.norm_type = norm\n",
    "        if (norm==\"instance\"):\n",
    "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
    "        elif (norm==\"batch\"):\n",
    "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reflection_pad(x)\n",
    "        x = self.conv_layer(x)\n",
    "        if (self.norm_type==\"None\"):\n",
    "            out = x\n",
    "        else:\n",
    "            out = self.norm_layer(x)\n",
    "        return out\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels=128, kernel_size=3):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x                     # preserve residual\n",
    "        out = self.relu(self.conv1(x))   # 1st conv layer + activation\n",
    "        out = self.conv2(out)            # 2nd conv layer\n",
    "        out = out + identity             # add residual\n",
    "        return out\n",
    "\n",
    "class DeconvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding, norm=\"instance\"):\n",
    "        super(DeconvLayer, self).__init__()\n",
    "\n",
    "        # Transposed Convolution \n",
    "        padding_size = kernel_size // 2\n",
    "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding_size, output_padding)\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.norm_type = norm\n",
    "        if (norm==\"instance\"):\n",
    "            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
    "        elif (norm==\"batch\"):\n",
    "            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_transpose(x)\n",
    "        if (self.norm_type==\"None\"):\n",
    "            out = x\n",
    "        else:\n",
    "            out = self.norm_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721ec58",
   "metadata": {},
   "source": [
    "### EncoderDecoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcef5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "   \n",
    "    def __init__(self):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.ConvBlock = nn.Sequential(\n",
    "            ConvLayer(3, 32, 9, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(32, 64, 3, 2),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(64, 128, 3, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ResidualBlock = nn.Sequential(\n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3), \n",
    "            ResidualLayer(128, 3)\n",
    "        )\n",
    "        self.DeconvBlock = nn.Sequential(\n",
    "            DeconvLayer(128, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            DeconvLayer(64, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(32, 3, 9, 1, norm=\"None\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ConvBlock(x)\n",
    "        x = self.ResidualBlock(x)\n",
    "        out = self.DeconvBlock(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59569537",
   "metadata": {},
   "source": [
    "#### Using Pretrained VGG16 with original Caffe weights trained of ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d8da2",
   "metadata": {},
   "source": [
    "For Feature extraction, outputs from layers 3,8,15,22 will be used to calculate the style loss and the content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c9bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, vgg_path=\"vgg16-00b39a1b.pth\"):\n",
    "        super(VGG16, self).__init__()\n",
    "        # Load VGG Skeleton, Pretrained Weights\n",
    "        vgg16_features = models.vgg16(pretrained=False)\n",
    "        vgg16_features.load_state_dict(torch.load(vgg_path), strict=False)\n",
    "        self.features = vgg16_features.features\n",
    "\n",
    "        # Turn-off Gradient History\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        layers = {'3': 'relu1_2', '8': 'relu2_2', '15': 'relu3_3', '22': 'relu4_3'}\n",
    "        features = {}\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in layers:\n",
    "                features[layers[name]] = x\n",
    "                if (name=='22'):\n",
    "                    break\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3e959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a812d",
   "metadata": {},
   "source": [
    "#### Calculating Gram Matrix to quantify style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70293c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram Matrix\n",
    "def gram(tensor):\n",
    "    B, C, H, W = tensor.shape\n",
    "    x = tensor.view(B, C, H*W)\n",
    "    x_t = x.transpose(1, 2)\n",
    "    return  torch.bmm(x, x_t) / (C*H*W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48e66d",
   "metadata": {},
   "source": [
    "#### Defining Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebec5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=224\n",
    "# Show image\n",
    "def show(img):\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = np.array(img/255).clip(0,1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "def saveimg(img, image_path):\n",
    "    img = img.clip(0, 255)\n",
    "    cv2.imwrite(image_path, img)\n",
    "\n",
    "# Preprocessing ~ Image to Tensor\n",
    "def image_to_tensor(img, max_size=None):\n",
    "    # Rescale the image\n",
    "    if (max_size==None):\n",
    "        itot_t = transforms.Compose([\n",
    "            #transforms.ToPILImage(),\n",
    "#             transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])    \n",
    "    else:\n",
    "        H, W, C = img.shape\n",
    "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
    "        itot_t = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])\n",
    "\n",
    "    # Convert image to tensor\n",
    "    tensor = itot_t(img)\n",
    "\n",
    "    # Add the batch_size dimension\n",
    "    tensor = tensor.unsqueeze(dim=0)\n",
    "    return tensor\n",
    "\n",
    "# Preprocessing ~ Tensor to Image\n",
    "def tensor_to_image(tensor):\n",
    "    \n",
    "    tensor = tensor.squeeze()\n",
    "    #img = ttoi_t(tensor)\n",
    "    img = tensor.cpu().numpy()\n",
    "    \n",
    "    # Transpose from [C, H, W] -> [H, W, C]\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c0116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(styles[3])\n",
    "# STYLE_IMAGE_PATH = \"images/mosaic.jpg\"\n",
    "style_datapath=\"C:\\\\Users\\\\awast\\\\Downloads\\\\artworks\"\n",
    "files = os.listdir(style_datapath)\n",
    "image_files = [file for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "styles = [Image.open(os.path.join(style_datapath, image_file)) for image_file in image_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce677e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awast\\anaconda3\\envs\\nlp\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\awast\\anaconda3\\envs\\nlp\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Epoch 1/2========\n",
      "========Iteration 1/2644========\n",
      "\tContent Loss:\t318455.59\n",
      "\tStyle Loss:\t35116648.00\n",
      "\tTotal Loss:\t35435104.00\n",
      "Time elapsed:\t0.4330017566680908 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_0.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_0.png\n",
      "========Iteration 101/2644========\n",
      "\tContent Loss:\t443173.94\n",
      "\tStyle Loss:\t18303304.47\n",
      "\tTotal Loss:\t18746478.42\n",
      "Time elapsed:\t19.906001567840576 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_100.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_100.png\n",
      "========Iteration 201/2644========\n",
      "\tContent Loss:\t497884.44\n",
      "\tStyle Loss:\t12354724.98\n",
      "\tTotal Loss:\t12852609.36\n",
      "Time elapsed:\t41.65099835395813 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_200.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_200.png\n",
      "========Iteration 301/2644========\n",
      "\tContent Loss:\t528413.81\n",
      "\tStyle Loss:\t9056620.03\n",
      "\tTotal Loss:\t9585034.00\n",
      "Time elapsed:\t64.04099941253662 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_300.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_300.png\n",
      "========Iteration 401/2644========\n",
      "\tContent Loss:\t541945.94\n",
      "\tStyle Loss:\t7124915.10\n",
      "\tTotal Loss:\t7666861.23\n",
      "Time elapsed:\t86.00199842453003 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_400.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_400.png\n",
      "========Iteration 501/2644========\n",
      "\tContent Loss:\t549526.12\n",
      "\tStyle Loss:\t5883144.50\n",
      "\tTotal Loss:\t6432670.89\n",
      "Time elapsed:\t106.42500185966492 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_500.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_500.png\n",
      "========Iteration 601/2644========\n",
      "\tContent Loss:\t551667.88\n",
      "\tStyle Loss:\t5023520.13\n",
      "\tTotal Loss:\t5575188.16\n",
      "Time elapsed:\t128.70699906349182 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_600.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_600.png\n",
      "========Iteration 701/2644========\n",
      "\tContent Loss:\t552154.69\n",
      "\tStyle Loss:\t4395545.15\n",
      "\tTotal Loss:\t4947699.80\n",
      "Time elapsed:\t150.28099918365479 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_700.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_700.png\n",
      "========Iteration 801/2644========\n",
      "\tContent Loss:\t551865.81\n",
      "\tStyle Loss:\t3915016.53\n",
      "\tTotal Loss:\t4466882.38\n",
      "Time elapsed:\t169.79999828338623 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_800.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_800.png\n",
      "========Iteration 901/2644========\n",
      "\tContent Loss:\t550806.62\n",
      "\tStyle Loss:\t3538889.52\n",
      "\tTotal Loss:\t4089696.02\n",
      "Time elapsed:\t191.6919994354248 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_900.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_900.png\n",
      "========Iteration 1001/2644========\n",
      "\tContent Loss:\t548280.94\n",
      "\tStyle Loss:\t3231877.38\n",
      "\tTotal Loss:\t3780158.30\n",
      "Time elapsed:\t212.31299901008606 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_1000.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_1000.png\n",
      "========Iteration 1101/2644========\n",
      "\tContent Loss:\t546295.19\n",
      "\tStyle Loss:\t2979039.42\n",
      "\tTotal Loss:\t3525334.25\n",
      "Time elapsed:\t235.52199959754944 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_1100.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_1100.png\n",
      "========Iteration 1201/2644========\n",
      "\tContent Loss:\t543385.50\n",
      "\tStyle Loss:\t2767548.43\n",
      "\tTotal Loss:\t3310933.75\n",
      "Time elapsed:\t257.0569987297058 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_1200.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_1200.png\n",
      "========Iteration 1301/2644========\n",
      "\tContent Loss:\t540923.25\n",
      "\tStyle Loss:\t2586080.65\n",
      "\tTotal Loss:\t3127003.54\n",
      "Time elapsed:\t280.925998210907 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_1300.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_1300.png\n",
      "========Epoch 2/2========\n",
      "========Iteration 1401/2644========\n",
      "\tContent Loss:\t537964.62\n",
      "\tStyle Loss:\t2429659.25\n",
      "\tTotal Loss:\t2967623.56\n",
      "Time elapsed:\t313.38552689552307 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_1400.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_1400.png\n",
      "========Iteration 1501/2644========\n",
      "\tContent Loss:\t535164.88\n",
      "\tStyle Loss:\t2293375.62\n",
      "\tTotal Loss:\t2828539.96\n",
      "Time elapsed:\t339.2143247127533 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_1500.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_1500.png\n",
      "========Iteration 1601/2644========\n",
      "\tContent Loss:\t532119.62\n",
      "\tStyle Loss:\t2173721.34\n",
      "\tTotal Loss:\t2705840.55\n",
      "Time elapsed:\t365.7892141342163 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_1600.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_1600.png\n",
      "========Iteration 1701/2644========\n",
      "\tContent Loss:\t529245.50\n",
      "\tStyle Loss:\t2066182.87\n",
      "\tTotal Loss:\t2595428.11\n",
      "Time elapsed:\t391.33286929130554 seconds\n",
      "Saved TransformerNetwork checkpoint file at C:/Users/awast/dashtoon/models/checkpoint_1700.pth\n",
      "Saved sample tranformed image at C:/Users/awast/dashtoon/images/out/sample2_1700.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# import vgg\n",
    "# import transformer\n",
    "# import utils\n",
    "\n",
    "TRAIN_IMAGE_SIZE = 224\n",
    "DATASET_PATH = \"C:\\\\Users\\\\awast\\\\Downloads\\\\dragon_ball\"\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "\n",
    "BATCH_SIZE = 4 \n",
    "CONTENT_WEIGHT = 1\n",
    "STYLE_WEIGHT = 10\n",
    "ADAM_LR = 0.001\n",
    "SAVE_MODEL_PATH = \"C:/Users/awast/dashtoon/models/\"\n",
    "SAVE_IMAGE_PATH = \"C:/Users/awast/dashtoon/images/out/\"\n",
    "SAVE_MODEL_EVERY = 100 # 2,000 Images with batch size 4\n",
    "SEED = 35\n",
    "train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [3000, 145])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def train():\n",
    "    # Seeds\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    # Device\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset and Dataloader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(TRAIN_IMAGE_SIZE),\n",
    "        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "   \n",
    "\n",
    "    # Load networks\n",
    "    TransformerNetwork = EncoderDecoder().to(device)\n",
    "    VGG =VGG16().to(device)\n",
    "\n",
    "    # Saving Style Features in Advance as they wont change and we wont have to recompute them\n",
    "    imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
    "    style_image = load_image(\"C:\\\\Users\\\\awast\\\\Downloads\\\\artworks\\\\Pablo_Picasso_381.jpg\")\n",
    "    style_tensor = image_to_tensor(style_image).to(device)\n",
    "    style_tensor = style_tensor.add(imagenet_neg_mean)\n",
    "    B, C, H, W = style_tensor.shape\n",
    "    style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
    "    style_gram = {}\n",
    "    for key, value in style_features.items():\n",
    "        style_gram[key] = gram(value)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
    "\n",
    "    # Loss trackers\n",
    "    content_loss_history = []\n",
    "    style_loss_history = []\n",
    "    total_loss_history = []\n",
    "    batch_content_loss_sum = 0\n",
    "    batch_style_loss_sum = 0\n",
    "    batch_total_loss_sum = 0\n",
    "\n",
    "    # Optimization/Training Loop\n",
    "    batch_count = 1\n",
    "    start_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"========Epoch {}/{}========\".format(epoch+1, NUM_EPOCHS))\n",
    "        for content_batch, _ in train_loader:\n",
    "            # Get current batch size in case of odd batch sizes\n",
    "            curr_batch_size = content_batch.shape[0]\n",
    "\n",
    "            # Free-up unneeded cuda memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Zero-out Gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate images and get features\n",
    "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
    "            generated_batch = TransformerNetwork(content_batch)\n",
    "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
    "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
    "\n",
    "            # Content Loss\n",
    "            MSELoss = nn.MSELoss().to(device)\n",
    "            content_loss = CONTENT_WEIGHT * MSELoss(generated_features['relu2_2'], content_features['relu2_2'])            \n",
    "            batch_content_loss_sum += content_loss\n",
    "\n",
    "            # Style Loss\n",
    "            style_loss = 0\n",
    "            for key, value in generated_features.items():\n",
    "                s_loss = MSELoss(gram(value), style_gram[key][:curr_batch_size])\n",
    "                style_loss += s_loss\n",
    "            style_loss *= STYLE_WEIGHT\n",
    "            batch_style_loss_sum += style_loss.item()\n",
    "\n",
    "            # Total Loss\n",
    "            total_loss = content_loss + style_loss\n",
    "            batch_total_loss_sum += total_loss.item()\n",
    "\n",
    "            # Backprop and Weight Update\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save Model and Print Losses\n",
    "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
    "                # Print Losses\n",
    "                \n",
    "                # Save Model\n",
    "                checkpoint_path = SAVE_MODEL_PATH + \"checkpoint_\" + str(batch_count-1) + \".pth\"\n",
    "                torch.save(TransformerNetwork.state_dict(), checkpoint_path)\n",
    "                \n",
    "\n",
    "                # Save sample generated image\n",
    "                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0)\n",
    "                sample_image = tensor_to_image(sample_tensor.clone().detach())\n",
    "                sample_image_path = SAVE_IMAGE_PATH + \"sample2_\" + str(batch_count-1) + \".png\"\n",
    "                saveimg(sample_image, sample_image_path)\n",
    "                print(\"Saved sample tranformed image at {}\".format(sample_image_path))\n",
    "\n",
    "                # Save loss histories\n",
    "                content_loss_history.append(batch_total_loss_sum/batch_count)\n",
    "                style_loss_history.append(batch_style_loss_sum/batch_count)\n",
    "                total_loss_history.append(batch_total_loss_sum/batch_count)\n",
    "\n",
    "            # Iterate Batch Counter\n",
    "            batch_count+=1\n",
    "\n",
    "\n",
    "    # Save TransformerNetwork weights\n",
    "    TransformerNetwork.eval()\n",
    "    TransformerNetwork.cpu()\n",
    "    final_path = SAVE_MODEL_PATH + \"transformer_weight.pth\"\n",
    "#     print(\"Saving TransformerNetwork weights at {}\".format(final_path))\n",
    "    torch.save(TransformerNetwork.state_dict(), final_path)\n",
    "#     print(\"Done saving final model\")\n",
    "\n",
    "#     # Plot Loss Histories\n",
    "#     if (PLOT_LOSS):\n",
    "#         utils.plot_loss_hist(content_loss_history, style_loss_history, total_loss_history)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea1fa8",
   "metadata": {},
   "source": [
    "#### Examples of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # Device\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load Transformer Network\n",
    "    net = EncoderDecoder()\n",
    "    net.load_state_dict(torch.load(SAVE_MODEL_PATH + \"transformer_weight.pth\"))\n",
    "    net = net.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "#         content_image_path = input(\"Enter the image path: \")\n",
    "        content_image = load_image(content_image_path)\n",
    "        \n",
    "        content_tensor = utils.image_to_t(content_image).to(device)\n",
    "        generated_tensor = net(content_tensor)\n",
    "        generated_image = tensor_to_image(generated_tensor.detach())\n",
    "        \n",
    "        show(generated_image)\n",
    "        saveimg(generated_image, \"generated.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bd889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3fb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
